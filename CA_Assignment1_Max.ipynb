{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Algorithms - Assignment 1 (30 points)\n",
    "Cognitive Algorithms        \n",
    "Summer Term 2019      \n",
    "Technische UniversitÃ¤t Berlin     \n",
    "Fachgebiet Maschinelles Lernen \n",
    "\n",
    "**Due on April 22, 2019 23:55 via ISIS **\n",
    "                  \n",
    "After completing all tasks, run the whole notebook so that the content of each cell is properly displayed. Make sure that the code was ran and the entire output (e.g. figures) is printed. Print the notebook as a PDF file and again make sure that all lines are readable - use line breaks in the Python Code '\\' if necessary. Points will be deducted, if code or content is not readable!                  \n",
    "           \n",
    "**Upload the PDF file that contains a copy of your notebook on ISIS.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:                     \n",
    "**Active** Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Math Recap (10 points)\n",
    "---\n",
    "The first part of this assignment is a linear algebra recap. Task 1 consists of multiple choice questions. For Task 2 you only need to write down the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (8 points)\n",
    "Please answer questions A) to H) and check the correct answer (using an 'x'). Here is an example:      \n",
    "This is a question?                  \n",
    "- [ ] wrong answer            \n",
    "- [ ] wrong answer            \n",
    "- [x] correct answer             \n",
    "- [ ] wrong answer            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** What is the scalar product of the following vectors $\\left( \\begin{array}{r} 1  \\\\  -2 \\\\ 0 \\end{array}\t\\right) ,  \\left( \\begin{array}{r} 3 \\\\  0 \\\\ 3 \\end{array}\t\\right)$?           \n",
    "- [ ] 3           \n",
    "- [ ] 5               \n",
    "- [ ] 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Let $\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^n$ be two column vectors. Which of the following statements is always true?      \n",
    "- [ ]  $\\mathbf{v}^T \\cdot \\mathbf{w} = \\mathbf{w}^T \\cdot \\mathbf{v}$            \n",
    "- [ ] $\\mathbf{v} \\cdot \\mathbf{w}^T = \\mathbf{w} \\cdot \\mathbf{v}^T$              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** The mapping $f: \\mathbb{R}^2 \\ni (x, y)^\\top \\mapsto (x + y, y- x)^\\top \\in \\mathbb{R}^2$ is given by the following matrix:            \n",
    "- [ ] $\\left( \\begin{array}{rr} 1 &  1  \\\\ \t1& -1 \\end{array}\t\\right)$         \n",
    "- [ ] $\\left( \\begin{array}{rr} 0 & 2  \\\\ \t-2& 0 \\end{array}\t\\right)$          \n",
    "- [ ] $\\left( \\begin{array}{rr} 1 & 1  \\\\ \t-1& 1 \\end{array}\t\\right)$           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Which property does matrix multiplication **not** have?            \n",
    "- [ ] Associativity:  $(AB)C = A(BC)$            \n",
    "- [ ] Commutativity:  $AB = BA$           \n",
    "- [ ] Distributivity: $(A+B)C = AC + BC$            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** Let $A \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix and $\\mathbf{v},\\mathbf{w} \\in \\mathbb{R}^n$ two column vectors with $A \\cdot \\mathbf{v} = \\mathbf{w}$. Which of the following statements is always true?          \n",
    "- [ ] $A = \\mathbf{w} \\cdot \\mathbf{v}^{-1}$           \n",
    "- [ ] $\\mathbf{v} = \\mathbf{w} \\cdot A^{-1}$            \n",
    "- [ ] $\\mathbf{v} = A^{-1} \\cdot \\mathbf{w}$               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F)** The rank of the matrix $\\left( \\begin{array}{rrr} 4 & 4 & 4 \\\\ 4 & 4 & 4 \\\\ 4 & 4 & 4 \\end{array}\t\\right)$ is         \n",
    "- [ ] 1        \n",
    "- [ ] 3             \n",
    "- [ ] 4               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G)** For a square $n \\times n$ matrix $A$ holds                \n",
    "- [ ] $rank(A) = n \\; \\Rightarrow \\; A$ is invertible, but there are invertible $A$ with $rank(A) \\neq n$           \n",
    "- [ ] $A$ is invertible $\\; \\Rightarrow \\; rank(A) = n$, but there are $A$ with $rank(A) = n$, which are not invertible.         \n",
    "- [ ] $rank(A) = n \\; \\iff \\; A$ is invertible             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**H)** Which of the following matrices is orthogonal:         \n",
    "- [ ] $\\left( \\begin{array}{rr} 0 & 1 \\\\ -1 & 0 \\end{array}\t\\right)$            \n",
    "- [ ] $\\left( \\begin{array}{rr} 1 & -1 \\\\ -1 & 1 \\end{array}\t\\right)$            \n",
    "- [ ] $\\left( \\begin{array}{rr} 1 & 1 \\\\ 1 & 0 \\end{array}\t\\right)$             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (2 points)\n",
    "Please replace '?' with the correct solution.                        \n",
    "We consider two functions $f$ and $g$ which transform an input vector $\\mathbf{x} = (x_1, \\dots , x_d)^T \\in \\mathbb{R}^d$ into scalars: $f(\\mathbf{x}) = \\mathbf{u}^T\\mathbf{x}$, $\\mathbf{u} = (u_1, \\dots, u_d)^T \\in \\mathbb{R}^d$ and $g(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$.                        \n",
    "Compute the gradient for $f$ and $g$.                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $\\nabla f(\\mathbf{x}) = (\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d})^T =$ ?                              \n",
    "+ $\\nabla g(\\mathbf{x}) = (\\frac{\\partial g(\\mathbf{x})}{\\partial x_1}, \\dots, \\frac{\\partial g(\\mathbf{x})}{\\partial x_d})^T =$ ?            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multiple Choice Questions (4 points)\n",
    "---\n",
    "In the lecture, you learned about the perceptron and the prototype classifier, which is also called the nearest centroid classifer (NCC).     \n",
    "Please answer questions A) to D) and check the correct answer (using an 'x').            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** The training data for a classification task is given by $(\\mathbf{x_1}, y_1),\\ldots, ( \\mathbf{x_n}, y_n ) \\in \\mathbb{R}^d \\times \\mathcal{C}$, where $\\mathcal{C}$ is the set of classes and ...:                \n",
    "- [ ] ... can be of infinite size             \n",
    "- [ ] ... is always defined as $\\mathcal{C} = \\{-1,+1\\}$               \n",
    "- [ ] ... neither of the above                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Let $\\mathbf{w}$ be the weight vector. The decision boundary is ...                   \n",
    "- [ ] orthogonal to $\\mathbf{w}$             \n",
    "- [ ] in the same direction as $\\mathbf{w}$               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** Let $\\mathbf{w}$ the weight vector. Which statement is true?                \n",
    "- [ ] $\\mathbf{w}$ and $\\mathbf{-w}$ yield the exact same classification               \n",
    "- [ ] $\\mathbf{w}$ and $\\mathbf{-w}$ do not yield the exact same classification              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Let $\\mathbf{w} = (1,1)^T$ and $b = 0$. Let $\\mathbf{x}_1, \\ldots \\mathbf{x_n} \\in \\mathbb{R}^2$ be the training data. Let $i = \\{1, \\ldots ,n \\}$ and $j \\in \\{1, 2\\}$. Which statement is true?              \n",
    "- [ ] all data points in the first quadrant ($x_{i,j} > 0$) are classified as $+1$             \n",
    "- [ ] all data points in the second quadrant ($x_{i,1} < 0$ and $x_{i,2} > 0$) are classified as $+1$                 \n",
    "- [ ] all data points in the third quadrant ($x_{i,j} < 0$) are classified as $+1$             \n",
    "- [ ] all data points in the fourth quadrant ($x_{i,1} > 0$ and $x_{i,2} < 0$) are classified as $+1$              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Programming (16 points)\n",
    "---\n",
    "**Please note that Python is in general slow with loops. Therefore, we will deduct points for the use of unnecessary loops throughout this course.**\n",
    "\n",
    "---\n",
    "The linear perceptron and the NCC are linear classification methods. Given training data\n",
    "$$(\\mathbf{x_1}, y_1),\\ldots, ( \\mathbf{x_n}, y_n ) \\in \\mathbb{R}^d \\times \\{-1,1\\}$$\n",
    "their goal is to learn a weight vector $\\mathbf{w}$ and a bias term $b$, such that each new data point $\\mathbf{x} \\in \\mathbb{R}^d$ will be assigned the correct class label via the following function:\n",
    "$$\\mathbf x \\mapsto  \\mbox{sign}(\\mathbf w^T \\cdot \\mathbf x - b)$$\n",
    "\n",
    "\n",
    "The two methods use different strategies to achieve this goal.\n",
    "You will programm and compare the perceptron and the prototype classifier and use them to predict handwritten digits. The task is to classify one digit against all others.         \n",
    "If not done yet, download the data set ```usps.mat``` from the ISIS web site. The data set ```usps.mat``` contains handwritten digits from the U.S. Postal Service data set. The data set contains 2007 images and each image consits of 256 pixels.           \n",
    "Below you can find some useful functions for loading the data and plotting images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.io as io\n",
    "import pylab as pl\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ---- Functions for loading and plotting the images ---- '''\n",
    "def load_usps_data(fname, digit=3):\n",
    "    ''' Loads USPS (United State Postal Service) data from <fname> \n",
    "    Definition:  X, Y = load_usps_data(fname, digit = 3)\n",
    "    Input:       fname   - string\n",
    "                 digit   - optional, integer between 0 and 9, default is 3\n",
    "    Output:      X       -  DxN array with N images with D pixels\n",
    "                 Y       -  1D array of length N of class labels\n",
    "                                 1 - where picture contains the <digit>\n",
    "                                -1 - otherwise                           \n",
    "    '''\n",
    "    # load the data\n",
    "    data = io.loadmat(fname)\n",
    "    # extract images and labels\n",
    "    X = data['data_patterns']\n",
    "    Y = data['data_labels']\n",
    "    Y = Y[digit,:]\n",
    "    return X, Y\n",
    "\n",
    "def plot_img(a):\n",
    "    ''' Plots one image \n",
    "    Definition: plot_img(a) \n",
    "    Input:      a - 1D array that contains an image \n",
    "    '''   \n",
    "    a2 = sp.reshape(a,(int(sp.sqrt(a.shape[0])), int(sp.sqrt(a.shape[0]))))\n",
    "    pl.imshow(a2, cmap='gray') \n",
    "    pl.colorbar()\n",
    "    pl.setp(pl.gca(), xticks=[], yticks=[])\n",
    "            \n",
    "def plot_imgs(X, Y):   \n",
    "    ''' Plots 3 images from each of the two classes \n",
    "    Definition:         plot_imgs(X,Y)\n",
    "    Input:       X       -  DxN array of N pictures with D pixel\n",
    "                 Y       -  1D array of length N of class labels {1, -1}                  \n",
    "    '''\n",
    "    pl.figure()\n",
    "    for i in sp.arange(3):\n",
    "        classpos = (Y == 1).nonzero()[0]\n",
    "        m = classpos[sp.random.random_integers(0, classpos.shape[0]-1)]\n",
    "        pl.subplot(2,3,1+i)\n",
    "        plot_img(X[:, m])\n",
    "    for i in sp.arange(3):\n",
    "        classneg = (Y != 1).nonzero()[0]\n",
    "        m = classneg[sp.random.random_integers(0, classneg.shape[0]-1)]\n",
    "        pl.subplot(2,3,4+i)\n",
    "        plot_img(X[:, m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) (6 points)** Implement a linear perceptron by completing the function stub  ```train_perceptron```. We will test three different types of update rules for the learning rate (```option``` $\\in \\{0,1,2\\}$).\n",
    "$$\\text{learning rate}(t) = \\begin{cases} \\frac{\\eta}{1+t} & \\text{if} \\;\\; \\text{option} = 0  \\\\ \\eta & \\text{if} \\;\\; \\text{option} = 1 \\\\ \\eta \\cdot (1+t) & \\text{if} \\;\\; \\text{option} = 2 \\end{cases}$$\n",
    "where $t$ is the current iteration and $\\eta$ the initial value of the learning rate.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X,Y,iterations=200,eta=.1, option=0):\n",
    "    ''' Trains a linear perceptron\n",
    "    Definition:  w, b, acc  = train_perceptron(X,Y,iterations=200,eta=.1)\n",
    "    Input:       X       -  DxN array of N data points with D features\n",
    "                 Y       -  1D array of length N of class labels {-1, 1}\n",
    "                 iter    -  optional, number of iterations, default 200\n",
    "                 eta     -  optional, learning rate, default 0.1\n",
    "                 option  -  optional, defines how eta is updated in each iteration\n",
    "    Output:      w       -  1D array of length D, weight vector \n",
    "                 b       -  bias term for linear classification                          \n",
    "                 acc     -  1D array of length iter, contains classification accuracies \n",
    "                            after each iteration  \n",
    "                            Accuracy = #correctly classified points / N \n",
    "    '''\n",
    "    assert option == 0 or option == 1 or option == 2\n",
    "    acc = sp.zeros((iterations))\n",
    "    #include the bias term by adding a row of ones to X \n",
    "    X = sp.concatenate((sp.ones((1,X.shape[1])), X))\n",
    "    #initialize weight vector\n",
    "    weights = sp.ones((X.shape[0]))/X.shape[0]\n",
    "    for it in sp.arange(iterations):\n",
    "        # indices of misclassified data\n",
    "        wrong = (sp.sign(weights.dot(X)) != Y).nonzero()[0]\n",
    "        # compute accuracy acc[it] (1 point)\n",
    "        acc[it] = 1 - (len(wrong)/len(Y))\n",
    "        if wrong.shape[0] > 0:\n",
    "            # pick a random misclassified data point (2 points)\n",
    "            index = wrong[0]\n",
    "            misclassified_dp = X[index]\n",
    "            #update weight vector (using different learning rates ) (each 1 point)\n",
    "            if option == 0:\n",
    "                weights = weights + (eta/(1+it)) * sp.dot(misclassified_dp, Y[index])\n",
    "            elif option == 1:\n",
    "                weights = weights + (eta) * sp.dot(misclassified_dp, Y[index])\n",
    "            elif option == 2:\n",
    "                weight = weights + (eta*(1+it)) * sp.dot(misclassified_dp, Y[index])\n",
    "    w = weights[1:]\n",
    "    #return weight vector, bias and accuracies\n",
    "    return w,b,acc\n",
    "\n",
    "''' --------------------------------------------------------------------------------- '''\n",
    "def analyse_accuracies_perceptron(digit = 3, option=0):\n",
    "    ''' Loads usps.mat data and plots digit recognition accuracy in the linear perceptron\n",
    "    Definition: analyse_perceptron(digit = 3)\n",
    "    '''\n",
    "    X,Y = load_usps_data('usps.mat',digit)\n",
    "    w_per,b_per,acc = train_perceptron(X,Y, option=option)\n",
    "    \n",
    "    pl.figure()\n",
    "    pl.plot(sp.arange(len(acc)),acc)\n",
    "    pl.title('Digit recognition accuracy')      \n",
    "    pl.xlabel('Iterations')\n",
    "    pl.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) (3 points)** Call the function ```analyse_accuracies_perceptron``` for a digit of your choice and all three possible ```options```. It plots the classification accuracy, i.e. the percentage of correctly classified data points, as a function of iterations. \n",
    "Does the accuracy converge (asymptotically)? What difference do you notice for the different update rules of the learning rate? Why? Which ```option``` would you prefer? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "(2007,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (257,) (2007,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-33ac197f4dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalyse_accuracies_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0manalyse_accuracies_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0manalyse_accuracies_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5f9f80960eb2>\u001b[0m in \u001b[0;36manalyse_accuracies_perceptron\u001b[0;34m(digit, option)\u001b[0m\n\u001b[1;32m     47\u001b[0m     '''\n\u001b[1;32m     48\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_usps_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'usps.mat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mw_per\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_per\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5f9f80960eb2>\u001b[0m in \u001b[0;36mtrain_perceptron\u001b[0;34m(X, Y, iterations, eta, option)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m#update weight vector (using different learning rates ) (each 1 point)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisclassified_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisclassified_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (257,) (2007,) "
     ]
    }
   ],
   "source": [
    "analyse_accuracies_perceptron(digit=3, option=0)\n",
    "analyse_accuracies_perceptron(digit=3, option=1)\n",
    "analyse_accuracies_perceptron(digit=3, option=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) (4 points)** Implement a Prototype/Nearest Centroid Classifier by completing the function stub ```train_ncc```. Note that points will be deducted for the use of loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ncc(X,Y):\n",
    "    ''' Trains a prototype/nearest centroid classifier\n",
    "    Definition:  w, b   = train_ncc(X,Y)\n",
    "    Input:       X       -  DxN array of N data points with D features\n",
    "                 Y       -  1D array of length N of class labels {-1, 1}\n",
    "    Output:      w       -  1D array of length D, weight vector  \n",
    "                 b       -  bias term for linear classification                          \n",
    "    '''\n",
    "    \n",
    "    # replace all -1 datapoints in Y with zero\n",
    "    Y[Y < 0] = 0\n",
    "    \n",
    "    # TODO: \n",
    "    # count number of 1 datapoints in Y\n",
    "    count = list.count(1)\n",
    "    \n",
    "    # centroid_1 = X*Y^T / count\n",
    "    # slide 5, UE_slides\n",
    "    centroid_1 = sp.dot(X, sp.transpose(Y)) / count\n",
    "    \n",
    "    # replace all zeros with -1 and all 1 with zero\n",
    "    Y[Y == 0] = -1\n",
    "    Y[Y > 0] = 0\n",
    "    \n",
    "    # count number of -1\n",
    "    count = list.count(-1)\n",
    "    \n",
    "    # centroid_2 = X*Y^T / count\n",
    "    # slide 5, UE_slides\n",
    "    centroid_2 = sp.dot(X, sp.transpose(Y)) / count\n",
    "    \n",
    "    # calculate weight vector\n",
    "    # slide 7, UE_slides\n",
    "    weights = centroid_1 + centroid_2\n",
    "    \n",
    "    print(\"Weight vector: \", weights)\n",
    "    \n",
    "    # calculate bias term\n",
    "    # slide 7, UE_slides\n",
    "    bias = 0.5*(sp.dot(centroid_1, sp.transpose(centroid_1)) - sp.dot(centroid_2, sp.transpose(centroid_2)))\n",
    "    \n",
    "    print(\"Bias term: \", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_histogram(X, Y, w, b):\n",
    "    ''' Plots a histogram of classifier outputs (w^T X) for each class with pl.hist \n",
    "    The title of the histogram is the accuracy of the classification\n",
    "    Accuracy = #correctly classified points / N \n",
    "    \n",
    "    Definition:     plot_histogram(X, Y, w, b)\n",
    "    Input:          X       -  DxN array of N data points with D features\n",
    "                    Y       -  1D array of length N of class labels\n",
    "                    w       -  1D array of length D, weight vector \n",
    "                    b       -  bias term for linear classification   \n",
    "    \n",
    "    '''\n",
    "    #Plot histogram (use pl.hist)                      +2 point (calc output, use hist)\n",
    "    pl.hist((w.dot(X[:,Y<0]), w.dot(X[:,Y>0])))\n",
    "    pl.xlabel(\"w^T X\") \n",
    "    pl.legend((\"non-target\",\"target\"))\n",
    "    #Title contains the accuracy                       +1 point (label,legend,title)\n",
    "    pl.title(\"Acc \" + str(100*sp.sum(sp.sign(w.dot(X)-b)==Y)/X.shape[1]) + \"%\")  \n",
    "    \n",
    "''' --------------------------------------------------------------------------------- '''\n",
    "def compare_classifiers(digit = 3):\n",
    "    ''' Loads usps.mat data, trains the perceptron and the Nearest centroid classifiers, \n",
    "    and plots their weight vector and classifier output\n",
    "    Definition: compare_classifiers(digit = 3)\n",
    "    '''\n",
    "    X,Y = load_usps_data('usps.mat',digit)\n",
    "    w_ncc,b_ncc = train_ncc(X,Y)\n",
    "    w_per,b_per,_ = train_perceptron(X,Y)\n",
    "    \n",
    "    pl.figure()\n",
    "    pl.subplot(2,2,1)\n",
    "    plot_img(w_ncc)\n",
    "    pl.title('NCC')\n",
    "    pl.subplot(2,2,3)\n",
    "    plot_histogram(X, Y, w_ncc, b_ncc)\n",
    "    \n",
    "    pl.subplot(2,2,2)\n",
    "    plot_img(w_per)\n",
    "    pl.title('Perceptron')\n",
    "    pl.subplot(2,2,4)\n",
    "    plot_histogram(X, Y, w_per, b_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) (3 points)** Call ```compare_classifiers``` for a digit of your choice. It plots, for both the perceptron and the nearest centroid classifier, the histogram of classifier outputs and the weight vector.                       \n",
    "Call the function several times for different digits. Do you notice a performance difference for the different digits? Why could this be? Show the histograms of the digits with highest difference in accuracy. Which algorithm (Nearest Centroid Classifier or Perceptron) would you prefer for this task? Why?                \n",
    "*Hint: The function ```plot_histogram``` calculates the classification accuracy and plots a histogram of classifier output $\\mathbf w^T \\mathbf x$ for each class. To do so, $X$ is sorted according to their labels and $w^T x$ is computed for each class. The accuracy of the algorithm is printed as the title of the plot.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Your answer for D]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... your code for D) here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
